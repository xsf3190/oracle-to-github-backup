CREATE OR REPLACE EDITIONABLE PACKAGE "PCK_BACKUP" is
    --
    PROCEDURE daily_job;
    --
end;
/
CREATE OR REPLACE EDITIONABLE PACKAGE BODY "PCK_BACKUP" is
    /*
    ** Generate schema datapump export
    */ 
    PROCEDURE datapump_backup(p_repo IN OUT NOCOPY CLOB) IS
        h1 NUMBER;
        l_email users.email%type;
        l_schema VARCHAR2(30):=sys_context('userenv','current_schema');
        l_filename VARCHAR2(60);
        l_schema_size NUMBER;
        l_status varchar2(4000);
        l_client_secret user_ords_clients.client_secret%type;
        l_json JSON_OBJECT_T;
        l_payload CLOB;
        l_clob CLOB;
        l_body_html CLOB:=
        '<!DOCTYPE html>
         <html>
            <head>
                <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
                <meta name="viewport" content="width=device-width, initial-scale=1.0">
                <meta name="format-detection" content="telephone=no">
                <meta name="format-detection" content="date=no">
                <meta name="format-detection" content="address=no">
                <meta name="format-detection" content="email=no">
                <style type="text/css">
                    body {
                        font-family: Georgia,sans-serif;
                        font-size: 1rem;
                    }
                    td,
                    th {
                        border: 1px solid rgb(190, 190, 190);
                        padding: 0.5rem;
                    }
                    table {
                        border-collapse: collapse;
                        border: 2px solid rgb(200, 200, 200);
                        width: 100%;
                    }
                    thead {
                        background-color: #d7d9f2;
                    }
                </style>
            </head>
        <body>';
    BEGIN
        pck_core.log('Starting datapump export job for Schema:'||l_schema);

        EXECUTE IMMEDIATE 'TRUNCATE TABLE DBTOOLS$EXECUTION_HISTORY';

        /* 
        ** Delete any files left over from previous operation to avoid ora-39001  
        */
        FOR C IN (SELECT object_name FROM DBMS_CLOUD.LIST_FILES('DATA_PUMP_DIR')) LOOP
            pck_core.log('File in DATA_PUMP_DIR:'||C.object_name);
            DBMS_CLOUD.DELETE_FILE(directory_name=>'DATA_PUMP_DIR',file_name=>C.object_name);
        END LOOP;

        /* 
        ** Drop any tables not needed in backup
        */
        FOR C IN (SELECT table_name FROM user_tables WHERE table_name IN ('HTMLDB_PLAN_TABLE')) LOOP
            EXECUTE IMMEDIATE 'DROP TABLE ' || C.table_name;
        END LOOP;

        /* 
        ** Get the schema workspace admin email
        */
        SELECT u.email, o.client_secret
          INTO l_email, l_client_secret
          FROM users u, apex_workspace_developers w, user_ords_clients o
         WHERE w.user_name=l_schema
           AND w.is_admin='Yes'
           AND w.email=u.email
           AND o.name(+)=w.email;

        IF (l_client_secret IS NULL) THEN
            OAUTH.create_client(
                p_name            => l_email,
                p_grant_type      => 'client_credentials',
                p_owner           => 'mark.russellbrown@gmail.com',
                p_support_email   => 'mark.russellbrown@gmail.com',
                p_description     => 'Provide secure secret in order to encrypt JWT tokens',
                p_privilege_names => 'api_priv'
            );
            SELECT client_secret INTO l_client_secret FROM user_ords_clients WHERE name=l_email;
        END IF;

        h1 := dbms_datapump.OPEN (operation => 'EXPORT', job_mode => 'SCHEMA', job_name => dbms_scheduler.generate_job_name('EXPORT_SCHEMA_'), version => 'COMPATIBLE'); 

        dbms_datapump.set_parameter(handle => h1, name => 'COMPRESSION', VALUE => 'ALL'); 
        dbms_datapump.set_parameter(handle => h1, name => 'COMPRESSION_ALGORITHM', VALUE => 'MEDIUM'); 
        dbms_datapump.set_parallel(handle => h1, degree => 1); 
        
        l_filename:='EXPORT_SCHEMA.' || l_schema ;
        dbms_datapump.add_file(handle => h1, filename =>  l_filename || '.dmp', 
            directory => 'DATA_PUMP_DIR', filesize => '50M',  filetype => DBMS_DATAPUMP.KU$_FILE_TYPE_DUMP_FILE, reusefile => 1); 
        dbms_datapump.add_file(handle => h1, filename => l_filename || '.log', 
            directory => 'DATA_PUMP_DIR', filetype => DBMS_DATAPUMP.KU$_FILE_TYPE_LOG_FILE, reusefile => 1); 
        
        dbms_datapump.set_parameter(handle => h1, name => 'KEEP_MASTER', VALUE => 0); 
        dbms_datapump.set_parameter(handle => h1, name => 'LOGTIME', VALUE => 'ALL'); 

        dbms_datapump.metadata_filter(handle => h1, name => 'SCHEMA_EXPR', VALUE => 'IN(''' || l_schema || ''')');
        dbms_datapump.metadata_filter(handle => h1, name => 'EXCLUDE_PATH_EXPR', VALUE => 'IN (''DBTOOLS$EXECUTION_HISTORY'',''STATISTICS'',''CLUSTER'',''DB_LINK'',''INDEXTYPE'',''PROCOBJ'',''JOB'',''SCHEDULER'')'); 

        dbms_datapump.set_parameter(handle => h1, name => 'ENCRYPTION_MODE', VALUE => 'PASSWORD'); 
        dbms_datapump.set_parameter(handle => h1, name => 'ENCRYPTION_PASSWORD', VALUE => l_client_secret);

        dbms_datapump.start_job(handle => h1, skip_current => 0, abort_step => 0); 
        dbms_datapump.wait_for_job( handle => h1, job_state => l_status);
        dbms_datapump.detach(handle => h1); 

        /* 
        ** Always send log file to github 
        */
        dbms_cloud_repo.put_file(repo=>p_repo, file_path=>l_filename || '.log', directory_name=>'DATA_PUMP_DIR');

        /* 
        ** Send COMLPETED dump file to Github and save encryption password for any subsequent import 
        */
        IF (l_status='COMPLETED') THEN
            dbms_cloud_repo.put_file(repo=>p_repo, file_path=>l_filename || '.dmp', directory_name=>'DATA_PUMP_DIR');
        END IF;

        /* 
        ** Send Email to Admin users 
        */
        l_body_html:=l_body_html || '<table role="presentation">';
        l_body_html:=l_body_html || '<thead><tr><th>File Name on Github</th><th>Compression</th><th>Created</th></tr></thead>';
        l_body_html:=l_body_html || '<tbody>';
        SELECT SUM(bytes) INTO l_schema_size FROM user_segments;
        FOR C IN (SELECT object_name, bytes, last_modified FROM DBMS_CLOUD.LIST_FILES('DATA_PUMP_DIR') WHERE object_name LIKE '%.dmp')
        LOOP
            l_body_html:=l_body_html || '<tr><td>' || C.object_name || '</td><td>' || 
            apex_string_util.to_display_filesize(l_schema_size) || ' --> ' || apex_string_util.to_display_filesize(C.bytes) || '<td>' || to_char(C.last_modified,'dd.mm.yyyy hh24:mi pm') || '</td></tr>';
        END LOOP;
        l_body_html:=l_body_html || '</tbody></table>';
        l_body_html:=l_body_html || '</html>';

        l_json:=new JSON_OBJECT_T;
        l_json.put('contactEmail',l_email);
        l_json.put('subject','Daily Backup Job ' || l_status);
        l_json.put('body', l_body_html);
        l_json.put('sourceEmail','backup@adfreesites.com');
        l_payload:=l_json.stringify;
        pck_api.callAWSemailAPI(pMethod=>'POST', pBody=>l_payload, pData=>l_clob);

        pck_core.log('Completed datapump schema exports. Final status:' || l_status);
    END;

    /*
    ** Generate and transfer to Github repository the following:
    ** 1. Apex applications
    ** 2. Apex static files
    ** 3. DDL of TABLES and PACKAGES
    */
    PROCEDURE github_backup(
        p_repo IN OUT NOCOPY CLOB,
        p_apex IN INTEGER)
    IS  
        TYPE t_backup IS RECORD(
            object_type         user_objects.object_type%type,
            object_name         user_objects.object_name%type);

        TYPE tt_backup IS TABLE OF t_backup;
        l_backup tt_backup; 
        
        l_clob CLOB;
        l_blob BLOB;
        l_file_path VARCHAR2(61);
        l_files apex_t_export_files;
        n PLS_INTEGER;
    BEGIN
        /*
        ** Ensure calls to DBMS_METADATA return nicely formatted code without storage and schema details
        */
        dbms_metadata.set_transform_param(dbms_metadata.session_transform,'PRETTY',true);
        dbms_metadata.set_transform_param(dbms_metadata.session_transform,'SQLTERMINATOR',true);
        dbms_metadata.set_transform_param(dbms_metadata.session_transform,'SEGMENT_ATTRIBUTES',false);
        dbms_metadata.set_transform_param(dbms_metadata.session_transform,'STORAGE',false);
        dbms_metadata.set_transform_param(dbms_metadata.session_transform,'TABLESPACE',false);
        dbms_metadata.set_transform_param(dbms_metadata.session_transform,'EMIT_SCHEMA',false);
        dbms_metadata.set_transform_param(dbms_metadata.session_transform,'COLLATION_CLAUSE','NEVER');

        /*
        ** Build PLSQL collection of all objects to be exported and copied to Github
        */
        IF (p_apex=0) THEN
            SELECT object_type, object_name
              BULK COLLECT INTO l_backup
             FROM
                (
                    SELECT object_type,object_name
                    FROM user_objects
                    WHERE generated='N'
                    AND object_type IN ('TABLE','PACKAGE')
                    UNION ALL 

                    SELECT 'GRANT','OBJECT_GRANT'
                    FROM user_tab_privs
                    WHERE grantee=sys_context('userenv','current_schema') 
                    AND grantor='ADMIN'
                    AND ROWNUM=1
                    UNION ALL

                    SELECT 'GRANT','SYSTEM_GRANT'
                    FROM user_tab_privs
                    WHERE grantee=sys_context('userenv','current_schema') 
                    AND grantor='ADMIN'
                    AND ROWNUM=1
                )
            ORDER BY object_type,object_name;
        ELSE
            SELECT object_type, object_name
              BULK COLLECT INTO l_backup
             FROM
                (
                    SELECT 'APEX_STATIC_FILE' object_type,f.application_id || '.' || f.file_name object_name
                    FROM apex_application_static_files f, apex_applications a
                    WHERE f.application_id=a.application_id
                    AND SUBSTR(f.mime_type,INSTR(f.mime_type,'/')+1) IN ('javascript','css')
                    AND INSTR(f.file_name,'.min.')=0
                    UNION ALL

                    SELECT 'ORDS_METADATA',sys_context('userenv','current_schema')
                    FROM dual
                )
            ORDER BY object_type,object_name;
        END IF;

        FOR i IN 1..l_backup.COUNT
        LOOP
            l_file_path:=l_backup(i).object_type||'.'||l_backup(i).object_name;
            pck_core.log('Backing up to github - ' || l_file_path);

            l_clob:='';

            CASE l_backup(i).object_type
                WHEN 'APEX_APPLICATION' THEN
                    l_files:=apex_export.get_application(p_application_id => l_backup(i).object_name, p_with_date=>true);
                    l_clob:=l_files(1).contents;

                WHEN 'APEX_STATIC_FILE' THEN
                    SELECT file_content 
                      INTO l_blob 
                      FROM APEX_APPLICATION_STATIC_FILES 
                     WHERE application_id=SUBSTR(l_backup(i).object_name,1,INSTR(l_backup(i).object_name,'.')-1) 
                       AND file_name=SUBSTR(l_backup(i).object_name,INSTR(l_backup(i).object_name,'.')+1);

                WHEN 'ORDS_METADATA' THEN
                    SELECT ords_metadata.ords_export.export_schema() 
                      INTO l_clob 
                      FROM dual;

                WHEN 'GRANT' THEN
                    dbms_metadata.set_transform_param(dbms_metadata.session_transform,'EMIT_SCHEMA',true);
                    l_clob:=LTRIM(LTRIM(dbms_metadata.get_granted_ddl(l_backup(i).object_name,sys_context('userenv','current_schema')),chr(13)||chr(10)));
                    l_clob:=REPLACE(l_clob,'"','');
                    dbms_metadata.set_transform_param(dbms_metadata.session_transform,'EMIT_SCHEMA',false);

                ELSE
                    l_clob:=LTRIM(LTRIM(dbms_metadata.get_ddl(l_backup(i).object_type,l_backup(i).object_name),chr(13)||chr(10)));
                    IF (l_backup(i).object_type='TABLE') THEN
                        SELECT COUNT(*) INTO n FROM dual WHERE EXISTS
                            (
                                SELECT null FROM user_tab_comments WHERE table_name=l_backup(i).object_name AND comments IS NOT NULL
                            )
                            OR EXISTS
                            (
                                SELECT null FROM user_col_comments WHERE table_name=l_backup(i).object_name AND comments IS NOT NULL
                            );                            
                        IF (n<>0) THEN
                            l_clob:=l_clob || chr(10) || LTRIM(LTRIM(dbms_metadata.get_dependent_ddl('COMMENT',l_backup(i).object_name),chr(13)||chr(10)));
                        END IF;
                        FOR C IN (SELECT dbms_metadata.get_ddl('INDEX', index_name, table_owner) ddl
                                    FROM user_indexes
                                   WHERE table_name=l_backup(i).object_name
                                     AND index_type<>'LOB')
                        LOOP
                            l_clob:=l_clob || chr(10) || C.ddl;
                        END LOOP;
                        FOR C IN (SELECT dbms_metadata.get_ddl('CONSTRAINT', constraint_name, owner) ddl
                                    FROM user_constraints
                                   WHERE table_name=l_backup(i).object_name
                                     AND constraint_type='C')
                        LOOP
                            l_clob:=l_clob || chr(10) || C.ddl;
                        END LOOP;  
                    END IF; 
            END CASE;

            IF (l_clob IS NOT NULL) THEN
                l_blob:=apex_util.clob_to_blob(l_clob);
            END IF;

            dbms_cloud_repo.put_file(repo=>p_repo, file_path=>l_file_path, contents=>l_blob);

        END LOOP;

        EXCEPTION
            WHEN OTHERS THEN
                pck_core.log_error;
                RAISE;
    END;

    /*
    **  Daily job run by dbms_scheduler
    */
    PROCEDURE daily_job IS 
        l_repo CLOB;
        n PLS_INTEGER;
    BEGIN
        /* Set Administrator user id for logging */
        FOR C IN (
            SELECT u.id
              FROM apex_workspace_apex_users w, users u 
             WHERE w.email=u.email
               AND w.is_admin='Yes'
               FETCH FIRST ROW ONLY
        ) LOOP
            pck_sec.g_session_user_id:=C.id;
        END LOOP;

        SELECT COUNT(*) INTO n
          FROM apex_applications
         WHERE owner=sys_context('userenv','current_schema');

        l_repo := dbms_cloud_repo.init_github_repo(
            credential_name => 'GITHUB_CRED', 
            owner => 'xsf3190', 
            repo_name => 'oracle-to-github-backup');

        IF (n=0) THEN
            datapump_backup(l_repo);
        END IF;
        
        github_backup(l_repo,n);

         /* Delete log data older than 24 hours */
        DELETE log WHERE log_date<SYSDATE-1;
        
        IF (n>0) THEN
            pck_fonts.loadGoogleFonts;
        END IF;

        EXCEPTION
            WHEN OTHERS THEN
                pck_core.log_error;
                RAISE;
    END;

end;
/